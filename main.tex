%%%%%%%% ICML 2025 PLASMID PAPER %%%%%%%%

\documentclass{article}

% Recommended packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\graphicspath{{graphics/}}
\usepackage{subfigure}
\usepackage{booktabs}

\usepackage{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for initial blind review:
\usepackage{styles/icml2025}

% If accepted, use:
% \usepackage[accepted]{styles/icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\icmltitlerunning{Emergent Biological Realism in RL-Trained DNA Language
Models}

\begin{document}

\twocolumn[
\icmltitle{Emergent Biological Realism in RL-Trained DNA Language Models}

\begin{icmlauthorlist}
\icmlauthor{Anonymous Author 1}{anon}
\icmlauthor{Anonymous Author 2}{anon}
\icmlauthor{Anonymous Author 3}{anon}
\icmlauthor{Anonymous Author 4}{anon}
\end{icmlauthorlist}

\icmlaffiliation{anon}{Anonymous Institution}

\icmlcorrespondingauthor{Anonymous}{anonymous@email.com}

\icmlkeywords{Machine Learning, DNA Language Models, Plasmid Design,
Reinforcement Learning, Group Relative Policy Optimization, Synthetic
Biology}

\vskip 0.3in
]

\printAffiliationsAndNotice{\icmlEqualContribution}

\begin{abstract}
Reinforcement learning has driven the mass adoption of
large language models by unlocking often unexpected emergent capabilities, yet
this approach remains largely underexplored for generative DNA models. We
investigate whether similar post-training techniques can induce emergent
biological realism in DNA language models, using plasmid generation as a
testbed due to plasmids' relative simplicity, well-characterized
functional constraints, and ubiquity in biotechnology. Using Group
Relative Policy Optimization with a reward function based on constraints from engineered biology, our model achieves 97\% quality control pass rate
compared to 6\% for the pretrained baseline. Remarkably, beyond
explicitly optimized features, the model exhibits emergent biological
parallels: generated sequences match natural plasmids in thermodynamic
stability, codon usage patterns, and ORF length distributions, properties
not directly encoded in the reward function. These results suggest that RL
post-training can steer DNA language models toward biologically coherent
regions of sequence space, analogous to how such techniques unlock
emergent capabilities in natural language models.
\end{abstract}

\section{Introduction}

Plasmids are extrachromosomal DNA sequences, often found in bacteria, capable of replication
independent of a host genome \cite{lederberg1952}. These genetic
elements are ubiquitous in biotechnology, serving as the primary vectors
for protein expression, gene editing, and emerging DNA therapeutics
\cite{plasmid_manufacturing, plasmid_therapeutics}. Despite their
widespread utility, plasmid engineering remains a complex,
high-dimensional optimization problem. Traditional workflows are cost
intensive and heuristic driven, often requiring iterative cycles of
manual sequence editing and experimental validation to resolve
structural instabilities \cite{structural_instability, plasmid_design_challenges1}. 
Suboptimal plasmid architectures, plagued
by incompatible regulatory elements or unstable repeat regions,
frequently lead to metabolic burden, reduced expression efficiency, and
manufacturing bottlenecks \cite{plasmid_design_challenges2,
metabolic_burden}.

Current approaches to plasmid design rely heavily on tacit domain
knowledge and piecemeal assembly of genetic parts. Designers must
simultaneously optimize for competing objectives such as copy number,
transcriptional output, and host viability while navigating the strict
biophysical constraints of DNA folding and context dependent regulatory
interactions \cite{gene_to_performance, clonefast}.

The dramatic success of reinforcement learning post-training in natural
language processing, such as improved reasoning, instruction following,
and unexpected generalization highlights the potential of RL post training in DNA language models \cite{emergent_abilities_of_large_language_models, emergent_rl}. We
investigate this using plasmid generation as a testbed, applying Group
Relative Policy Optimization to the PlasmidGPT foundation model
\cite{plasmidgpt}. Beyond dramatically improving quality control pass
rates (97\% vs. 6\% baseline), we observe emergent properties not
explicitly optimized: generated sequences match natural plasmids in
thermodynamic stability, codon usage patterns, and ORF length
distributions. These results suggest that RL post-training may unlock
similar emergent capabilities in genomic models as it has in language
models, steering generation toward biologically coherent sequence space
regions through reward-guided optimization.

\section{Background}

\subsection{DNA Language Models}

Natural language processing and genomics have developed in parallel,
often with algorithms developed for bioinformatics used in NLP and vice
versa \cite{durbin}. Natural language models pretrained on massive
amounts of data have displayed emergent capabilities and remarkable
utility by understanding the structure of language as a whole \cite{emergent_abilities_of_large_language_models}. 
These capabilities have vastly increased the utility of language models 
in a wide range of tasks, and are one of the primary reasons why 
large language model adoption has skyrocketed in the last few years.
This has inspired the application of many similar techniques to 
genomic language models, resulting in models capable of impressive performance on
biological tasks including variant prediction \cite{dnabert},
transcription factor binding site identification \cite{hyenadna}, and
even whole-genome generation \cite{evo}.

Plasmid DNA has received relatively little attention compared to other
sequence types despite its importance in biomanufacturing and 
research. OriGen \cite{origen} introduces a generative model to produce
previously undiscovered origins of replication (ORIs) but does not model
whole sequences. PlasmidGPT \cite{plasmidgpt, angus} uses modern language
modeling techniques to develop a generative model for whole plasmid
sequences, and later work expands on this by synthesizing whole plasmids
generated using a fine tuned version of the PlasmidGPT model
\cite{angus}. We use this as a base model and apply post-training
techniques to improve results significantly.

\subsection{Plasmid Design}

Lab-designed plasmids are short circular DNA molecules (typically 2-15
kb) that must contain multiple functional components arranged in precise
configurations. At minimum, a viable plasmid requires: (i) an origin of
replication to enable autonomous replication, (ii) a selection
marker (e.g., antibiotic resistance gene) for identifying successfully
transformed cells, and (iii) a cloning site where genes of interest can
be inserted.

The search space of valid plasmids is massive due to combinatorial
explosion across components \cite{combinatorial_explosion}. There are many types of each component on
the scale of hundreds to thousands per component, and additional
regulatory elements (promoters, terminators, enhancers) and reporters
may be required depending on the application. Multiple instances of
certain components may be necessary, and the ordering and spacing of
these elements significantly impacts function. Beyond sheer
combinatorics, designers must navigate complex biological constraints
including compatibility requirements (specific ORIs only function in
certain hosts), physical stability issues (repeat regions can fold and
bind to each other), and other design challenges \cite{plasmid_design_challenges1, plasmid_design_challenges2}.

\section{Methods}

We follow an established method that starts with a base model,
fine-tunes it using supervised fine-tuning (SFT), and then applies
reinforcement learning (RL) to optimize for specific attributes in the
output (Figure~\ref{fig:methods_overview}).

\begin{figure*}[!t]
\vskip 0.2in
\begin{center}
\includegraphics[width=\textwidth]{plasmid_llm_diagram.png}
\caption{Overview of the Plasmid-RL training pipeline. Starting from the
pretrained PlasmidGPT base model, we apply supervised fine-tuning on
curated plasmid sequences, followed by reinforcement learning with Group
Relative Policy Optimization using a biologically-motivated reward
function that evaluates functional annotations, length constraints, and
repeat content.}
\label{fig:methods_overview}
\end{center}
\vskip -0.2in
\end{figure*}

\subsection{Supervised Fine-Tuning}

Supervised fine-tuning (SFT) was performed on a curated corpus of
\textit{E. coli} plasmid sequences assembled from PlasmidScope and
Addgene \cite{addgene, angus}. After deduplication and quality filtering, approximately 15k
circular plasmids ($\leq$30\,kb) were retained, excluding linear
entries, fragments, and incomplete records. Sequences were tokenized
using the original PlasmidGPT byte-pair DNA tokenizer. The pretrained
PlasmidGPT model was fine-tuned using an autoregressive next-token
prediction objective with gradient accumulation and learning-rate warmup
over three epochs.

\subsection{Reinforcement Learning with GRPO}

We implement a configurable reinforcement learning pipeline for plasmid
design that uses Group Relative Policy Optimization (GRPO) \cite{grpo}
with a domain-specific reward function described below. At each training
iteration, the model generates a batch of candidate plasmids via
autoregressive rollouts conditioned on short nucleotide prompts. Prompts
are either stochastic (4--25 bp random seeds, excluding ''ATG'', used to
promote rollout diversity) or structured (partial ''cassette'' seeds
encoding canonical marker genes such as antibiotic-resistance or
fluorescent reporters). Each candidate sequence is evaluated via our
reward function, which captures structural plausibility, cassette
organization, repeat content, and other biologically motivated
constraints. GRPO is then applied to update the model parameters using
these sequence-level rewards, enabling the policy to progressively shift
toward generating plasmids with higher predicted validity.

\subsection{Reward Function Design}

The reward function scores each generated plasmid according to its
structural plausibility and expected stability. It is composed of three
conceptual components:

\textbf{Functional annotation scoring:} Lightweight annotations identify
origins of replication, promoters, terminators, coding sequences (CDS),
and selectable markers, which are then scored according to a
configuration designed with subject matter expert (SME) input to reflect
biologically reasonable quantities (e.g., exactly one origin of
replication and at least one selectable marker). CDS regions are
identified using Pyrodigal \cite{prodigal}, a gene prediction tool that
detects coding sequences based on statistical patterns rather than
homology search. To encourage coherent gene cassettes, this component
also includes a location-aware bonus for promoter $\rightarrow$ CDS
$\rightarrow$ terminator arrangements that appear in the correct order
and within a reasonable proximity window.

\textbf{Length prior:} A length prior favors plasmid sizes within
typical experimental ranges (5--15 kb) preferred for plasmid
construction by giving a linearly increasing reward for lengths between 5 and 15 kb 
with a maximum reward at 5 kb and no reward for sequences longer than 15 kb.

\textbf{Repeat penalty:} A repeat penalty down-weights sequences
containing long exact repeats that are associated with instability or
recombination, specifically penalizing .1 reward for each repeat of length 50 bp or greater.

These terms are combined into a single scalar in $[0,1]$, yielding a
fast and interpretable proxy for ''plasmid-likeness'' and validity during
reinforcement learning.

\section{Experiments}

\subsection{Plasmid Quality Control and Uniqueness}

We evaluate three model variants on held-out prompts not seen during
training. For each model, we sampled 50 rollouts with two prompts: (i) a
minimal prompt (single ATG codon) to test unconditional generation
capability, and (ii) a structured prompt containing a complete GFP
expression cassette to test the model's ability to build around provided
components. These prompts were deliberately excluded from the training
corpus to ensure evaluation reflects generalization rather than
memorization.

\subsubsection{Validity Assessment}

In silico plasmid validity was assessed using a bioinformatics
quality-control pipeline that leverages BLAST based tools, requiring
exactly one origin of replication ($\geq$95\% identity and coverage),
one or two antimicrobial resistance genes ($\geq$99\% identity and
coverage), and no internal repeats longer than 50\,bp \cite{blast}. This
pipeline has been validated as a reliable proxy for experimental
synthesis success \cite{angus}. While we do not perform wet-lab
validation in this work, our focus is on establishing that RL
post-training can successfully navigate the plasmid design space in
silico, laying groundwork for future conditional generation systems where
user-specified designs can be experimentally validated.

\subsubsection{Uniqueness Assessment}

To assess whether generated plasmids represent genuinely new designs
rather than minor variants of existing constructs, we compute similarity
to known sequences using the NCBI BLASTn API. Each generated plasmid is
assigned to one of three categories based on identity and query-coverage
thresholds: sequences with $\geq$99\% identity and $\geq$95\% coverage
are classified as \textbf{Exists}; those with $\geq$95\% identity and
$\geq$80\% coverage are \textbf{Similar}; and all others are classified
as \textbf{Novel}.

This categorization is based on large scale data curation efforts such
as PLSDB that use these thresholds of similarity to attempt to
de-duplicate plasmids for not adding any additional value to a dataset
\cite{PLSDB}.

\subsubsection{Diversity Assessment}

To detect and prevent model collapse, we attempt to measure the
diversity of the many samples of the model from the same prompt. Due to
the lack of utility of traditional NLP metrics on this task, we use the
mean Pairwise Jaccard distance of the 21-mers of each sequence.
Diversity of a group of rollouts is calculated as follows:

\[D = 1 - \frac{1}{\binom{n}{2}} \sum_{i=1}^{n} \sum_{j=i+1}^{n} J(S_i, S_j)\]

where $J(S_i, S_j)$ is the Jaccard similarity between MinHash
sketches of sequences $i$ and $j$, and $n$ is the number of sequences
in the group.

The diversity metric (pairwise Jaccard distance) serves primarily as a
model collapse detector rather than a biological validity measure. The
RL model achieves 0.391 diversity compared to 0.926 for the base model,
indicating that while the RL model concentrates probability mass on
higher-quality regions of sequence space, it does not collapse to
identical outputs. This is consistent with RL optimization finding and using
conserved ''successful motifs'' (e.g., proven origins of replication,
reliable resistance markers) while still maintaining sequence-level
uniqueness.

\subsubsection{Results}

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{fig02_pass_rate_by_prompt.png}
\caption{Summary of QC outcomes by prompt. Base model is only able to
generate functional plasmids with a strong prompt. SFT allows the model
to overcome this limitation on occasion but still often fails QC. Adding
RL to the training process improves pass rate dramatically.}
\label{fig:qc_breakdown}
\end{center}
\vskip -0.2in
\end{figure}

\begin{table}[t]
\caption{Novelty and diversity metrics across model variants. RL
achieves dramatically higher quality (97\% pass rate) with reduced diversity in samples.}
\label{tab:qc_results}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lccc}
\toprule
Model & QC Pass Rate & \% Novel & Diversity\\
\midrule
Base  & 6\%  & 95.5\% & 0.926 \\
SFT & 11\%  & 100\% & 0.886 \\
RL & 97\%  & 88\% & 0.391 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

Reinforcement learning substantially increases the probability of
generating plasmids that pass our bioinformatics quality control (QC)
pipeline, while supervised fine-tuning provides modest improvements.
Figure~\ref{fig:qc_breakdown} shows pass rates by prompt type, and
Table~\ref{tab:qc_results} summarizes novelty and diversity metrics
across models.

When prompted with the weak ATG prompt, the base model never produces a
valid plasmid (0\%), whereas SFT increases the pass rate to 16\%, and RL
further increases it to 96\%.
A similar trend holds with the stronger GFP-cassette prompt: the base
model achieves 12\%, SFT drops to 4\%, but the RL-optimized model
reaches 76\%.

Aggregated across strong and weak prompts, the overall QC pass rate
rises from 6\% with the base model to 11\% with the SFT model and
finally to 97\% with the RL model, representing more than an
order-of-magnitude improvement in validity relative to the pretrained
baseline.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{fig19_novelty_percentage.pdf}
\caption{Summary of plasmid novelty as measured by comparison to NCBI
database. SFT improves novelty of generated sequences from both base
model and RL post-trained model.}
\label{fig:novelty}
\end{center}
\vskip -0.2in
\end{figure}

Importantly, this increase does not come from the RL models repeating
previously known, high scoring sequences. Among passing sequences, the
proportion classified as novel remains substantial for all models
(Figure~\ref{fig:novelty}). Using our novelty thresholds, the RL model
produces 88\% novel plasmids among its QC-passing samples, compared to
95.5\% for the base model. The SFT model generated 100\% novel sequences.
When normalized over all 100 rollouts per model, the RL model produces
85.4\% sequences that are both QC-valid and novel, compared to 11\% for
SFT and 6\% for the base model.

Taken together, these results show that RL post-training not only
dramatically improves biological plausibility but also preserves
meaningful novelty relative to published plasmids. The model learns to
satisfy constraints without collapsing to memorized or trivial
constructs, suggesting that sequence-level RL can push generation toward
realistic design regions while still exploring new areas of plasmid
space.

\subsection{Distribution Comparison}

We compute several statistics known to be relevant to the performance of
DNA from the raw sequences of both the generated plasmids and a small
subset of real plasmids used for protein expression, genome editing, 
and other applications. See full details of plasmids used in the appendix. We calculate
the following summary statistics: sequence length distribution, GC
content, longest open reading frame (ORF; calculated two ways),
Jensen-Shannon divergence of the codon distribution, and Gibbs free
energy (as calculated by ViennaRNA \cite{viennarna}).

\begin{figure*}[!t]
\vskip 0.1in
\begin{center}
\includegraphics[width=0.85\textwidth]{fig_distribution_grid_v2.png}
\caption{Distribution comparison across key biophysical metrics. The RL
post-trained model (red) closely matches real plasmid distributions
(green) across sequence length, GC content, ORF length, codon usage
(Jensen-Shannon divergence), and thermodynamic stability (Gibbs free
energy), while the base model (blue) and SFT model (purple) show
substantial deviations. Notably, RL optimization produces realistic
distributions even for metrics not explicitly encoded in the reward
function.}
\label{fig:distributions}
\end{center}
\vskip -0.2in
\end{figure*}

Figure~\ref{fig:distributions} shows that the RL post-trained model's
samples much more closely match the distributions of the real plasmids
than the pretrained and supervised models, even when the metric is not
directly encoded by the reward function. Kolmogorov-Smirnov statistics
comparing model-generated distributions to real plasmids demonstrate
this quantitatively: for GC content, RL achieves a KS statistic of 0.51
(closest to real at 0.52) compared to base (0.48) and SFT (0.50); for
codon usage measured by Jensen-Shannon divergence, RL achieves 0.06,
identical to real plasmids and substantially better than base (0.10) and
SFT (0.09). See Appendix for complete distribution comparison metrics.

Sequence length is directly encoded by the reward function, with optimal
reward determined by a parameter sweep for training stability. GC
content is not directly encoded, but regions selected for by the reward
function likely have GC content distributed similarly to real plasmids. 
Despite generally making up a smaller percentage of the whole sequence, GC
content is partially encoded by the reward function, as regions with higher GC content are more likely to be rewarded.

ORF length and codon distribution are not factored into the reward
function directly. We calculate ORF length two ways: (1) maximum length
of codons that are not stop codons on a single strand, and (2) longest
stretch of non-stop codons after the presence of a start codon on either
strand. These methods are disjoint from the method used to account for
ORF in the reward function, which uses Prodigal \cite{prodigal} to
predict and reward correctly placed ORFs. The ORF length, measured by
either method, converges closely to the distribution of the real
plasmids.

The same pattern holds for codon distribution and Gibbs free energy.
While not accounted for in the reward function, the RL model learns to
generate tokens with a much more similar codon distribution to real
plasmids than the two models that have seen the correct distribution in
the training data. The similarity in the distribution of free energy
measurements is particularly remarkable given that not only is free
energy not optimized for directly, but no structural components are
factored in at all excluding the weakly correlated repeat penalty 
originally included to solve recombination issues.

\subsection{Held-Out Continuation}

To evaluate how RL training affects nucleotide-level predictive
performance, we measure how well each model can predict future bases
given a real plasmid prefix. For each sequence, we provide the first 400
nucleotides as a prompt, have each model produce the next 100
nucleotides, and then compute the average log-probability of the true
next 100 bases under each model. This allows us to compare the Base and
RL models against real plasmids in terms of next-token prediction
accuracy.

\begin{table}[t]
\caption{Average log-probability on held-out continuation task. Higher
values indicate better next-token prediction. RL shows unexpected
improvement despite not being optimized for this task.}
\label{tab:continuation}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcc}
\toprule
Model & Mean Log-Prob & Std Dev \\
\midrule
Base & -12.449 & 6.144 \\
RL   & -11.148 & 2.977 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

The RL model shows improved log-probability compared to the base model
(Figure~\ref{fig:completion}), with the standard deviation shrinking
substantially from 6.144 to 2.977. This improvement is unexpected, as RL
generally makes language models worse at next-token prediction tasks, a
phenomenon known as the "alignment tax" \cite{alignment_tax}.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{fig08_completion_benchmark.png}
\caption{Held-out continuation performance across models. The RL model
shows improved log-probability on real plasmid sequences, demonstrating
better alignment with natural plasmid structure despite not being
explicitly optimized for next-token prediction.}
\label{fig:completion}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Coding Sequence Surprisal Analysis}

The reward function evaluates CDS regions using Prodigal rather than
bioinformatics token-level approaches, reducing the risk of token-level
information leakage. Because Prodigal predicts coding regions based only
on generic statistical patterns and not on specific plasmid sequences,
the reward signal is biologically grounded but distribution-agnostic.
Interestingly, the RL-trained model achieves lower surprisal on real
plasmids than the pretrained model (Figure~\ref{fig:surprisal}),
suggesting that the Prodigal-based reward sharpens the model's
understanding of natural plasmid structure.

\begin{figure}[t]
\vskip 0.2in
\begin{center}
\includegraphics[width=\columnwidth]{fig09_surprisal_benchmark.png}
\caption{Coding sequence surprisal analysis. The RL model achieves lower
surprisal on real plasmid coding sequences compared to the base and SFT
models, indicating better capture of natural coding patterns despite
using only Prodigal-based structural rewards.}
\label{fig:surprisal}
\end{center}
\vskip -0.2in
\end{figure}

\section{Discussion}

\subsection{Summary: Emergent Biological Realism in RL-Trained Models}

Our key finding is that RL post-training induces
emergent biological realism in DNA language models. Beyond the dramatic
improvement in quality control pass rates (97\% vs. 6\%), the RL model
exhibits properties not explicitly encoded in the reward function:
realistic thermodynamic stability (Gibbs free energy distributions
matching natural plasmids), natural codon usage patterns (Jensen-Shannon
divergence from real plasmids significantly lower than base model), and
appropriate ORF length distributions. The emergence of biologically
realistic properties, reflecting those of real plasmids, mirrors
evolutionary processes that produce complex, correlated traits as
byproducts of selecting for primary fitness criteria.

Furthermore, the RL model shows unexpected improvement on next-token
prediction tasks, reversing the typical ``alignment tax'' observed in
language models. This suggests the model is not simply memorizing valid
solutions but rather learning general principles of plasmid structure.
The steering vector hypothesis offers a potential explanation: RL may be
moving the model's latent space toward regions that are not only valid by
our reward criteria but also broadly biologically coherent, explaining why
unoptimized properties improve alongside explicitly rewarded features.

\subsection{Implications for Computational Biology and Genomics}

This work demonstrates that modern post-training techniques from natural
language processing can transfer to DNA language models with similar
success. Just as instruction tuning and RLHF unlocked the practical
utility of large language models, RL post-training may represent a key
step toward practical DNA generative models. The success of our approach
suggests that appropriate reward function design, even without perfect
biological accuracy (e.g., using lightweight bioinformatics rather than
wet-lab validation), can guide models toward broadly realistic sequence
spaces.

The parallel between emergent capabilities in LLMs and DNA models has
broader implications: it suggests that genomic foundation models may
benefit from similar training paradigms that have proven successful in
other domains. This opens avenues for applying other techniques from the
NLP toolkit (e.g., chain-of-thought prompting, mixture of experts,
parameter-efficient fine-tuning) to genomic tasks. Moreover, the emergent
properties we observe suggest that DNA language models may be capturing
fundamental biological constraints implicit in natural sequence data,
much as LLMs capture linguistic structure.

\subsection{Limitations and Critiques}

\textbf{Bioinformatics-only evaluation:} Our training and evaluation
process relies almost entirely on bioinformatics, which depends on
libraries of known regions. These libraries are naturally incomplete.
Therefore, if our model generates a novel functional sequence (e.g., a
never-before-seen ORI), it will not receive reward, sharply limiting how
creative the model can be. This contrasts with other domains where RL has
succeeded: natural language processing uses preference models that can
evaluate novel outputs, and protein design uses biophysical simulators.
Without wet-lab validation, we cannot definitively claim our sequences are
experimentally viable, though our pipeline has been validated as a proxy
for synthesis success.

\textbf{Diversity trade-offs:} The post-trained model shows decreased
sequence diversity (0.391 vs. 0.926 for base model), a known effect of RL
optimization. While the diversity metric primarily serves as a collapse
detector rather than a biological validity measure, there is a meaningful
reduction in functional diversity. For example, when sampling 500
sequences from each model, the base model uses 10 unique ORIs while the
RL model uses only 7. The model converges toward conserved, reliable
solutions at the expense of exploring less common but potentially viable
alternatives. This is consistent with RL finding ``successful motifs''
but may limit the model's utility for applications requiring high
functional diversity.

\textbf{Assumptions about reward function design:} Our reward function
design assumes that structural annotations (promoters, ORIs, etc.) and
simple composition metrics can capture essential aspects of plasmid
validity. While QC pass rates and wet lab validation of said QC process suggest this assumption is partially
validated, we may be missing important biological constraints not easily
encoded in lightweight bioinformatics. The simplification is pragmatic
for efficient training but may become limiting as we scale to more
complex genomic systems.

\subsection{Future Work}

This work further supports the ongoing hypothesis that modern language 
modeling techniques can be applied to DNA language models in a similar 
fashion to how they have been applied to natural language in the past. 
The real utility of natural language models came from instruction tuning 
them to respond to questions and follow directions. This work shows that post-
training techniques can be applied to DNA language models to achieve similar results.

We hope to build out a dataset designed for conditional generation where
the user can prompt the model with the specifics of the plasmid they
want, and the model will develop it from there. This would enable
practical use cases such as ``design a plasmid for expressing protein X
in E. coli with high copy number'' or ``create a mammalian expression
vector with constitutive GFP expression.''

In addition to being far more practical than unconditional generation, the
conditioning will promote more diversity of samples and make evaluation
of the model's performance on the task of plasmid design much more
understandable.

\section{Conclusion}

We demonstrate that reinforcement learning can mimic distributions of real plasmids 
across several key features, even when not explicitly optimized for these features. Our model
generates structurally valid plasmids at a 97\% rate (compared to 6\% for
the base model), but more significantly, exhibits biological realism not
directly encoded in the reward function: realistic thermodynamic
stability, natural codon usage patterns, and appropriate ORF length
distributions. These parallels to natural plasmids emerge from optimizing
only functional annotations, length constraints, and repeat penalties.

This finding suggests that RL post-training operates similarly across
modalities by steering models toward coherent, naturalistic regions of
their respective sequence spaces. Just as RL unlocked reasoning and
generalization in language models beyond their pretraining objectives, it
guides DNA models toward biologically realistic sequences beyond their
explicit rewards. The unexpected improvement in next-token prediction
further supports this interpretation: the model learns general principles
of plasmid structure rather than memorizing specific solutions.

These findings open several directions for future work. First, extending
to conditional generation (e.g., ``express protein X in E. coli with high
copy number'') would enable practical applications and experimental
validation of model-generated sequences. Second, applying RL post-training
to other genomic models could test whether emergent biological realism generalizes across
applications. Finally, investigating what reward function properties induce
such emergence could provide principles for designing effective biological
reward functions, potentially accelerating the broader application of RL
to computational biology.

\section*{Impact Statement}

This paper presents work whose goal is to advance the field of
computational biology and machine learning for biological sequence
design. The ability to generate novel, valid plasmid sequences could
accelerate research in synthetic biology, biomanufacturing, and
therapeutic development. While the immediate applications are primarily
beneficial for scientific research, we acknowledge the dual-use nature
of synthetic biology tools. The methods described here are limited to
generating plasmid sequences based on patterns in existing databases and
do not enable the design of harmful organisms without substantial
additional effort and expertise. We believe the benefits to research
efficiency and accessibility outweigh the potential risks, particularly
given existing biosafety regulations and oversight mechanisms in
synthetic biology research.

\section*{Code Availability}

Code for model training and evaluation will be released upon publication.
Training data is derived from PlasmidScope and Addgene databases,
available under their respective licenses.


\newpage

\appendix

\section{Training Configuration Details}

\subsection{Supervised Fine-Tuning Hyperparameters}

\begin{itemize}
\item \textbf{Batch size:} 1
\item \textbf{Learning rate:} $5 \times 10^{-5}$ with 500 warmup steps
\item \textbf{Optimizer:} AdamW
\item \textbf{Epochs:} 3
\item \textbf{Gradient accumulation steps:} 8
\item \textbf{Hardware:} [single/multi] NVIDIA L4 GPU(s)
\end{itemize}

\subsection{Reinforcement Learning (GRPO) Hyperparameters}

\begin{itemize}
\item \textbf{Rollout batch size:} 50
\item \textbf{Policy learning rate:} $3.55 \times 10^{-5}$ (found via
hyperparameter sweep)
\item \textbf{GRPO group size:} 16
\item \textbf{Training steps:} Varies between 1000-2500 (scheduled for
2500 but rarely reached due to early convergence)
\item \textbf{Convergence criteria:} Reward plateau or dip indicating
collapse
\item \textbf{Prompt types:} Random 4--25bp seeds (excluding ATG) and
structured cassette seeds
\item \textbf{Total training time:} $\sim$10--20 hours on NVIDIA L4 GPU
\end{itemize}

\subsection{Reward Function Configuration}

\begin{itemize}
\item \textbf{Origin of replication (exactly 1):} Weight = 1.0
\item \textbf{Selectable markers ($\geq$1):} Weight = 1.5
\item \textbf{Promoter$\rightarrow$CDS$\rightarrow$terminator bonus:}
Weight = 1.5
\item \textbf{Length prior (2--15kb):} Weight = 0.6, linear with maximum
reward at 5kb and minimum at 15kb, zero reward beyond 15kb
\item \textbf{Repeat penalty ($>$50bp):} $-0.1$ from total reward for
each repeat $>$ 50bp
\end{itemize}

\begin{algorithm}[h]
\caption{Plasmid Reward Scoring}
\begin{algorithmic}[1]
\STATE \textbf{Input:} DNA sequence $s$
\STATE \textbf{Output:} Reward score $R \in [0,1]$
\STATE
\STATE Annotate features using PlasmidKit $\rightarrow A(s)$
\STATE Merge overlapping features of same type (threshold = 0.8)
\STATE
\FOR{each component $i \in \{$ori, promoter, terminator, marker, cds$\}$}
\STATE Count features $n_i$ in $A(s)$
\IF{$n_i < \min_i$}
\STATE $\text{score}_i \leftarrow 0.5 \times (n_i / \min_i)$
\ELSIF{$n_i > \max_i$}
\STATE $\text{score}_i \leftarrow \text{violation\_penalty\_factor}$ \COMMENT{$= 1.0$}
\ELSE
\STATE $\text{score}_i \leftarrow 1.0$
\ENDIF
\STATE \textit{Exception: marker is binary (present $\rightarrow 1.0$, absent $\rightarrow 0.0$)}
\ENDFOR
\STATE
\STATE \textbf{Compute weighted base score:}
\STATE base $\leftarrow \sum w_i \times \text{score}_i$
\STATE
\STATE \textbf{Detect repeat regions:}
\STATE Find all $k$-mers ($k=50$) appearing $\geq 2$ times (including reverse complements)
\STATE Merge overlapping occurrences $\rightarrow R$ regions
\STATE repeat\_penalty $\leftarrow |R| \times 0.1$
\STATE
\RETURN $R \leftarrow \text{clip}(\text{base} - \text{repeat\_penalty}, 0, 1)$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{CDS Cassette Bonus Subroutine}
\begin{algorithmic}[1]
\STATE \textbf{Input:} Annotated features $A(s)$, base score$_{\text{cds}}$
\STATE \textbf{Output:} Updated score$_{\text{cds}}$
\STATE
\FOR{each promoter $p$}
\STATE Find nearest downstream CDS $c$ on same strand
\STATE Find nearest downstream terminator $t$ on same strand
\STATE Award points for:
\STATE \hspace{0.5cm} $p \rightarrow c$ correct order on same strand: $+5$ pts
\STATE \hspace{0.5cm} $p \rightarrow c$ within 300bp: $+5$ pts
\STATE \hspace{0.5cm} $c \rightarrow t$ correct order on same strand: $+5$ pts
\STATE \hspace{0.5cm} $c \rightarrow t$ within 300bp: $+5$ pts
\ENDFOR
\STATE Take top $K=2$ cassettes $\rightarrow$ total cassette\_points
\STATE cassette\_bonus $\leftarrow 0.5 \times (\text{cassette\_points} / 40)$
\RETURN score$_{\text{cds}} \leftarrow \text{clip}(\text{score}_{\text{cds}} + \text{cassette\_bonus}, 0, 1)$
\end{algorithmic}
\end{algorithm}

\subsection{Distribution Comparison Metrics}

Table~\ref{tab:ks_stats} presents Kolmogorov-Smirnov statistics
comparing the distributions of generated plasmids to real plasmids
across key biophysical metrics. Lower values indicate closer match to
the real plasmid distribution.

\begin{table}[h]
\caption{Kolmogorov-Smirnov statistics for distribution comparison
against real plasmids. RL model achieves closest match for GC content
and codon usage (JS divergence).}
\label{tab:ks_stats}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccc}
\toprule
Metric & Real & Base & SFT & RL \\
\midrule
Length (bp) & 4737 & 5548 & 5647 & 7551 \\
GC Content & 0.52 & 0.48 & 0.50 & 0.51 \\
JS Divergence & 0.06 & 0.10 & 0.09 & 0.06 \\
MFE Density & -0.36 & -0.35 & -0.37 & -0.35 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\subsection{Reference Plasmids for Distribution Comparison}

The following common laboratory plasmids were used for distribution
comparison and benchmarking analyses in Section 4.2: pUC19 \cite{puc19},
pBluescript \cite{pbluescript}, pBR322 \cite{pbr322}, pACYC184
\cite{pacyc184}, pBAD24 \cite{pbad24}, pEGFP \cite{pegfp}, pGEX-4T-1
\cite{pgex}, pET-28a \cite{pet28a}, pcDNA3 \cite{pcdna3}, and px330
\cite{px330}. These plasmids represent widely-used vectors spanning
diverse applications and size ranges (2.7--8.1 kb), providing a
representative benchmark for evaluating the biological realism of
generated sequences.

\bibliography{plasmid_references}
\bibliographystyle{styles/icml2025}

\end{document}